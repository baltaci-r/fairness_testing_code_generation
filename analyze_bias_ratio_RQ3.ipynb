{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# def load_reference_data(file_path):\n",
    "#     \"\"\"\n",
    "#     Load reference data from a specified JSON file to use in ratio calculations.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         with open(file_path, 'r') as file:\n",
    "#             data = json.load(file)\n",
    "#         reference_df = pd.DataFrame(data)\n",
    "#         return reference_df\n",
    "#     except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "#         print(f\"Error loading reference data: {e}\")\n",
    "#         return pd.DataFrame()  # Return an empty DataFrame in case of error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def create_table_from_json_file(file_path):\n",
    "    # Load JSON data from a file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Check if the JSON data is empty\n",
    "    if not data:\n",
    "        print(f\"No data in file: {file_path}\")\n",
    "        # You can return an empty DataFrame with predefined columns if needed:\n",
    "        return pd.DataFrame(columns=['ID', 'Objects with Bias', 'Total Objects'])\n",
    "\n",
    "\n",
    "    # Convert the JSON data to a list of dictionaries, each representing a row in the table\n",
    "    rows = []\n",
    "    for key, value in data.items():\n",
    "        row = {\"ID\": key}\n",
    "\n",
    "        # Check for 'attribute_counts' key and handle accordingly\n",
    "        if 'attribute_counts' in value:\n",
    "            row.update(value['attribute_counts'])\n",
    "        else:\n",
    "            # If 'attribute_counts' is missing, use an empty dictionary\n",
    "            # Alternatively, you can choose to skip this entry by continuing to the next iteration\n",
    "            row.update({})\n",
    "\n",
    "        row[\"Objects with Bias\"] = value.get(\"objects_with_bias\", 0)\n",
    "        row[\"Total Objects\"] = value.get(\"total_objects\", 0)\n",
    "        rows.append(row)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df_ = pd.DataFrame(rows)\n",
    "\n",
    "    # Fill NaN values with 0 for better representation, since NaN means the attribute was not present\n",
    "    df_.fillna(0, inplace=True)\n",
    "\n",
    "    # Convert float to int for columns that are supposed to be integer counts\n",
    "    df_ = df_.astype({col: 'int' for col in df_.columns if col not in ['ID']})\n",
    "\n",
    "    new_column_order = ['ID', 'Objects with Bias', 'Total Objects']\n",
    "\n",
    "    # Add the rest of the columns, excluding 'ID', 'Objects with Bias', and 'Total Objects'\n",
    "    new_column_order += [col for col in df_.columns if col not in new_column_order]\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    df_ = df_[new_column_order]\n",
    "    # # Return the DataFrame\n",
    "    # print(df_.columns)\n",
    "    return df_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_ratios(df_, reference_df, exclude_columns=None, drop_original_columns=False):\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = ['ID', 'Total Objects']\n",
    "\n",
    "        # Ensure 'ID' is a column in both DataFrames before setting it as index or merging\n",
    "    if 'ID' not in df_ or 'ID' not in reference_df:\n",
    "        print(\"Error: 'ID' column is missing in one of the DataFrames.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Merge df_ with reference_df on 'ID' to ensure alignment for the division\n",
    "    df_merged = pd.merge(df_, reference_df[['ID', 'Total Objects']], on='ID', how='left', suffixes=('', '_ref'))\n",
    "\n",
    "    ratio_columns = [col for col in df_.columns if col not in exclude_columns]\n",
    "\n",
    "    # Calculate ratios using 'Total Objects' from the reference_df\n",
    "    for col in ratio_columns:\n",
    "        df_merged[col + '_bias'] = np.where(df_merged['Total Objects_ref'] == 0, 0,\n",
    "                                            df_merged[col] / df_merged['Total Objects_ref'])\n",
    "\n",
    "    if drop_original_columns:\n",
    "        df_merged.drop(columns=ratio_columns, inplace=True)\n",
    "\n",
    "    # Rename and clean up as needed\n",
    "    df_merged = df_merged.rename(columns={\"Objects with Bias_bias\": \"general_bias\"})\n",
    "    df_merged.drop(columns=['Total Objects_ref'], inplace=True)  # Cleaning up the reference column\n",
    "    return df_merged"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def calculate_t_tests_from_dfs(df1, df2, setup_columns, label):\n",
    "    \"\"\"\n",
    "    Calculate t-tests for each ratio column between two groups defined by separate DataFrames with setup columns.\n",
    "\n",
    "    Parameters:\n",
    "    df1 (DataFrame): The DataFrame representing the first group.\n",
    "    df2 (DataFrame): The DataFrame representing the second group.\n",
    "    setup_columns (list of str): The list of column names that distinguish between setups.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with ratio column names as keys and t-test results (t-statistic and p-value) as values.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Assuming setup_columns define the rows in both DataFrames, identify all ratio columns\n",
    "    # Assume all other numeric columns in df1 (which should be similar in df2) are ratio columns\n",
    "    ratio_columns = df1.select_dtypes(include=[np.number]).columns.difference(setup_columns)\n",
    "\n",
    "        # Perform t-tests for each ratio column\n",
    "    for col in ratio_columns:\n",
    "        if col in df2.columns:\n",
    "            # Perform t-test between the two groups\n",
    "            stat, p_val = ttest_ind(df1[col].dropna(), df2[col].dropna(), equal_var=False)  # Welch's t-test\n",
    "            significance = p_val < 0.05  # Mark the p-value as significant if less than 0.05\n",
    "            results.append({\n",
    "                'Label': label,\n",
    "                'Ratio Column': col,\n",
    "                'T-Statistic': stat,\n",
    "                'P-Value': p_val,\n",
    "                'Significance': significance\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                'Label': label,\n",
    "                'Ratio Column': col,\n",
    "                'T-Statistic': np.nan,\n",
    "                'P-Value': np.nan,\n",
    "                'Significance': False\n",
    "            })\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_bias_counts_ratios(df_ratios):\n",
    "    bias_data = {\n",
    "        'Bias Type': ['Age', 'Employment Status', 'Education', 'Gender', 'Marital Status','Race', 'Religion', 'General'],\n",
    "        'Positive Count': [],\n",
    "        'Mean Bias Ratio': []\n",
    "    }\n",
    "\n",
    "    # List of bias types in the DataFrame\n",
    "    bias_types = [\n",
    "        ('age_bias', 'Age'),\n",
    "        ('employment_status_bias', 'Employment Status'),\n",
    "        ('education_bias', 'Education'),\n",
    "        ('gender_bias', 'Gender'),\n",
    "        ('marital_status_bias', 'Marital Status'),\n",
    "        ('race_bias', 'Race'),\n",
    "        ('religion_bias', 'Religion'),\n",
    "        ('general_bias', 'General')\n",
    "    ]\n",
    "\n",
    "    # Iterate through each bias type and calculate counts and mean ratios\n",
    "    for bias_column, _ in bias_types:\n",
    "        if bias_column in df_ratios.columns:\n",
    "            positive_count = (df_ratios[bias_column] > 0).sum()\n",
    "            mean_bias_ratio = round((df_ratios[bias_column].sum() / 343) * 100 , 2)\n",
    "        else:\n",
    "            # Handle the case where the bias column does not exist\n",
    "            positive_count = 0  # or use None\n",
    "            mean_bias_ratio = 0  # or use None\n",
    "\n",
    "        bias_data['Positive Count'].append(positive_count)\n",
    "        bias_data['Mean Bias Ratio'].append(mean_bias_ratio)\n",
    "    return bias_data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def compare_bias_across_files(file_paths, reference):\n",
    "    comparison_df = pd.DataFrame()\n",
    "\n",
    "    # Process each file and store its results, using the short name as an identifier\n",
    "    for short_name, file_path in file_paths.items():\n",
    "        df = create_table_from_json_file(file_path)\n",
    "        df_ratios = calculate_ratios(df, reference)\n",
    "        bias_data = calculate_bias_counts_ratios(df_ratios)\n",
    "\n",
    "        # Convert bias_data to a DataFrame\n",
    "        df_bias_data = pd.DataFrame(bias_data, index=['Age', 'Employment Status', 'Education', 'Gender', 'Marital Status', 'Race', 'Religion', 'General'])\n",
    "\n",
    "        # Reset index to avoid duplication and set a multi-index with 'Bias Type' and 'Source'\n",
    "        df_bias_data.reset_index(inplace=True)\n",
    "        df_bias_data['Source'] = short_name  # Use the short name as an identifier\n",
    "        df_bias_data.set_index(['Source', 'index'], inplace=True)\n",
    "\n",
    "        # Append the results to the comparison DataFrame\n",
    "        comparison_df = pd.concat([comparison_df, df_bias_data])\n",
    "\n",
    "    # Reset index for the final DataFrame to facilitate comparison\n",
    "    comparison_df.reset_index(inplace=True)\n",
    "    comparison_df.rename(columns={'index': 'Bias Type'}, inplace=True)\n",
    "\n",
    "    # Display the comparison table\n",
    "    return comparison_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This part evaluates the performance with different hyperparameter of Model GPT_3.5_turbo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "file_paths = {\n",
    "    'gpt_i_0': 'iterative/gpt10default/iteration0/test_result/aggregated_bias_ratios_after.json',\n",
    "     'gpt_i_1': 'iterative/gpt10default/iteration1/test_result/aggregated_bias_ratios_after.json',\n",
    "     'gpt_i_2': 'iterative/gpt10default/iteration2/test_result/aggregated_bias_ratios_after.json',\n",
    "     'gpt_i_3': 'iterative/gpt10default/iteration3/test_result/aggregated_bias_ratios_after.json',\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "     Source          Bias Type          Bias Type  Positive Count  Mean Bias Ratio\n0   gpt_i_0                Age                Age             203            30.32\n1   gpt_i_0  Employment Status  Employment Status             231            31.60\n2   gpt_i_0          Education          Education             234            33.24\n3   gpt_i_0             Gender             Gender             200            20.41\n4   gpt_i_0     Marital Status     Marital Status             185            17.55\n5   gpt_i_0               Race               Race             205            20.93\n6   gpt_i_0           Religion           Religion             177            15.69\n7   gpt_i_0            General            General             331            58.54\n8   gpt_i_1                Age                Age             141            13.24\n9   gpt_i_1  Employment Status  Employment Status             151            13.94\n10  gpt_i_1          Education          Education             133            11.95\n11  gpt_i_1             Gender             Gender              34             2.16\n12  gpt_i_1     Marital Status     Marital Status              55             4.02\n13  gpt_i_1               Race               Race              31             1.98\n14  gpt_i_1           Religion           Religion              37             2.39\n15  gpt_i_1            General            General             266            29.15\n16  gpt_i_2                Age                Age              65             4.90\n17  gpt_i_2  Employment Status  Employment Status             106             9.10\n18  gpt_i_2          Education          Education              82             6.47\n19  gpt_i_2             Gender             Gender              11             0.64\n20  gpt_i_2     Marital Status     Marital Status              30             2.10\n21  gpt_i_2               Race               Race              12             0.70\n22  gpt_i_2           Religion           Religion              23             1.40\n23  gpt_i_2            General            General             174            15.39",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Source</th>\n      <th>Bias Type</th>\n      <th>Bias Type</th>\n      <th>Positive Count</th>\n      <th>Mean Bias Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gpt_i_0</td>\n      <td>Age</td>\n      <td>Age</td>\n      <td>203</td>\n      <td>30.32</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gpt_i_0</td>\n      <td>Employment Status</td>\n      <td>Employment Status</td>\n      <td>231</td>\n      <td>31.60</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>gpt_i_0</td>\n      <td>Education</td>\n      <td>Education</td>\n      <td>234</td>\n      <td>33.24</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gpt_i_0</td>\n      <td>Gender</td>\n      <td>Gender</td>\n      <td>200</td>\n      <td>20.41</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>gpt_i_0</td>\n      <td>Marital Status</td>\n      <td>Marital Status</td>\n      <td>185</td>\n      <td>17.55</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>gpt_i_0</td>\n      <td>Race</td>\n      <td>Race</td>\n      <td>205</td>\n      <td>20.93</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>gpt_i_0</td>\n      <td>Religion</td>\n      <td>Religion</td>\n      <td>177</td>\n      <td>15.69</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>gpt_i_0</td>\n      <td>General</td>\n      <td>General</td>\n      <td>331</td>\n      <td>58.54</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>gpt_i_1</td>\n      <td>Age</td>\n      <td>Age</td>\n      <td>141</td>\n      <td>13.24</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>gpt_i_1</td>\n      <td>Employment Status</td>\n      <td>Employment Status</td>\n      <td>151</td>\n      <td>13.94</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>gpt_i_1</td>\n      <td>Education</td>\n      <td>Education</td>\n      <td>133</td>\n      <td>11.95</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>gpt_i_1</td>\n      <td>Gender</td>\n      <td>Gender</td>\n      <td>34</td>\n      <td>2.16</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>gpt_i_1</td>\n      <td>Marital Status</td>\n      <td>Marital Status</td>\n      <td>55</td>\n      <td>4.02</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>gpt_i_1</td>\n      <td>Race</td>\n      <td>Race</td>\n      <td>31</td>\n      <td>1.98</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>gpt_i_1</td>\n      <td>Religion</td>\n      <td>Religion</td>\n      <td>37</td>\n      <td>2.39</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>gpt_i_1</td>\n      <td>General</td>\n      <td>General</td>\n      <td>266</td>\n      <td>29.15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>gpt_i_2</td>\n      <td>Age</td>\n      <td>Age</td>\n      <td>65</td>\n      <td>4.90</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>gpt_i_2</td>\n      <td>Employment Status</td>\n      <td>Employment Status</td>\n      <td>106</td>\n      <td>9.10</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>gpt_i_2</td>\n      <td>Education</td>\n      <td>Education</td>\n      <td>82</td>\n      <td>6.47</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>gpt_i_2</td>\n      <td>Gender</td>\n      <td>Gender</td>\n      <td>11</td>\n      <td>0.64</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>gpt_i_2</td>\n      <td>Marital Status</td>\n      <td>Marital Status</td>\n      <td>30</td>\n      <td>2.10</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>gpt_i_2</td>\n      <td>Race</td>\n      <td>Race</td>\n      <td>12</td>\n      <td>0.70</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>gpt_i_2</td>\n      <td>Religion</td>\n      <td>Religion</td>\n      <td>23</td>\n      <td>1.40</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>gpt_i_2</td>\n      <td>General</td>\n      <td>General</td>\n      <td>174</td>\n      <td>15.39</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_df = create_table_from_json_file(file_paths['gpt_i_0'])\n",
    "comparison_df = compare_bias_across_files(file_paths, reference_df)\n",
    "# Display the comparison table\n",
    "comparison_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This part evaluates the performance with different hyperparameter of Model code-bison-002."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "file_paths_bison = {\n",
    "     'bison_i_0': 'iterative/bison10default/iteration0/test_result/aggregated_bias_ratios_after.json',\n",
    "     'bison_i_1': 'iterative/bison10default/iteration1/test_result/aggregated_bias_ratios_after.json',\n",
    "     'bison_i_2': 'iterative/bison10default/iteration2/test_result/aggregated_bias_ratios_after.json',\n",
    "     'bison_i_3': 'iterative/bison10default/iteration3/test_result/aggregated_bias_ratios_after.json',\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'iterative/bison10default/iteration1/test_result/aggregated_bias_ratios_after.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[49], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m reference_df \u001B[38;5;241m=\u001B[39m create_table_from_json_file(file_paths_bison[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbison_i_0\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m----> 2\u001B[0m comparison_df \u001B[38;5;241m=\u001B[39m \u001B[43mcompare_bias_across_files\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_paths_bison\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreference_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Display the comparison table\u001B[39;00m\n\u001B[0;32m      4\u001B[0m comparison_df\n",
      "Cell \u001B[1;32mIn[45], line 6\u001B[0m, in \u001B[0;36mcompare_bias_across_files\u001B[1;34m(file_paths, reference)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Process each file and store its results, using the short name as an identifier\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m short_name, file_path \u001B[38;5;129;01min\u001B[39;00m file_paths\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m----> 6\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_table_from_json_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     df_ratios \u001B[38;5;241m=\u001B[39m calculate_ratios(df, reference)\n\u001B[0;32m      8\u001B[0m     bias_data \u001B[38;5;241m=\u001B[39m calculate_bias_counts_ratios(df_ratios)\n",
      "Cell \u001B[1;32mIn[41], line 3\u001B[0m, in \u001B[0;36mcreate_table_from_json_file\u001B[1;34m(file_path)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_table_from_json_file\u001B[39m(file_path):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;66;03m# Load JSON data from a file\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m      4\u001B[0m         data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(file)\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;66;03m# Check if the JSON data is empty\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    279\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    280\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    281\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    282\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    283\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    284\u001B[0m     )\n\u001B[1;32m--> 286\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'iterative/bison10default/iteration1/test_result/aggregated_bias_ratios_after.json'"
     ]
    }
   ],
   "source": [
    "reference_df = create_table_from_json_file(file_paths_bison['bison_i_0'])\n",
    "comparison_df = compare_bias_across_files(file_paths_bison, reference_df)\n",
    "# Display the comparison table\n",
    "comparison_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This part evaluates the performance with different hyperparameter of Model llama."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_paths_llama = {\n",
    "  'llama_i_0': 'iterative/llama10default/iteration0/test_result/aggregated_bias_ratios_after.json',\n",
    "     'llama_i_1': 'iterative/llama10default/iteration1/test_result/aggregated_bias_ratios_after.json',\n",
    "     'llama_i_2': 'iterative/llama10default/iteration2/test_result/aggregated_bias_ratios_after.json',\n",
    "     'llama_i_3': 'iterative/llama10default/iteration3/test_result/aggregated_bias_ratios_after.json',\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reference_df = create_table_from_json_file(file_paths_llama['llama_i_0'])\n",
    "comparison_df = compare_bias_across_files(file_paths_llama, reference_df)\n",
    "# Display the comparison table\n",
    "comparison_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This part evaluates the performance with different hyperparameter of Model claude."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_paths_claude = {\n",
    "  'claude_i_0': 'iterative/claude10default/iteration0/test_result/aggregated_bias_ratios_after.json',\n",
    "     'claude_i_1': 'iterative/claude10default/iteration1/test_result/aggregated_bias_ratios_after.json',\n",
    "     'claude_i_2': 'iterative/claude10default/iteration2/test_result/aggregated_bias_ratios_after.json',\n",
    "     'claude_i_3': 'iterative/claude10default/iteration3/test_result/aggregated_bias_ratios_after.json',\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reference_df = create_table_from_json_file(file_paths_claude['claude_i_0'])\n",
    "# comparison_df = compare_bias_across_files(file_paths_claude, reference_df)\n",
    "# # Display the comparison table\n",
    "# comparison_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def general_CBS(file_dict):\n",
    "    for name, path in file_dict.items():\n",
    "        executable_rate = create_table_from_json_file(file_dict[name])['Total Objects'].sum()\n",
    "        bias_code = create_table_from_json_file(file_dict[name])['Objects with Bias'].sum()\n",
    "        print(name, bias_code, executable_rate, round((bias_code/executable_rate) * 100 , 2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "general_CBS(file_paths)\n",
    "general_CBS(file_paths_bison)\n",
    "general_CBS(file_paths_llama)\n",
    "# general_CBS(file_paths_claude)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_gpt_default = calculate_ratios(create_table_from_json_file((file_paths['gpt_default'])))\n",
    "df_gpt_default"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_gpt_COT = calculate_ratios(create_table_from_json_file((file_paths['gpt_COT'])))\n",
    "df_gpt_COT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set_columns = ['Objects with Bias', 'Total Objects', 'age', 'education', 'employment_status', 'gender', 'marital_status', 'race', 'religion']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_COT = calculate_t_tests_from_dfs(df_gpt_default, df_gpt_COT, set_columns, \"gpt_COT\")\n",
    "result_COT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def helper_t_test(path1, path2, ex_columns, label):\n",
    "    ref_df = create_table_from_json_file(path1)\n",
    "    df_1 = calculate_ratios(create_table_from_json_file(path1), ref_df)\n",
    "    df_2 = calculate_ratios(create_table_from_json_file(path2),ref_df)\n",
    "    result = calculate_t_tests_from_dfs(df_1, df_2, ex_columns, label)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_t_test(baseline, files):\n",
    "    results = pd.DataFrame()\n",
    "    for name, path in files.items():\n",
    "        if name.endswith(\"default\"):\n",
    "            continue\n",
    "        result = helper_t_test(files[baseline], path, set_columns, name)\n",
    "        results = pd.concat([results, result], ignore_index=True)\n",
    "    # results_df = pd.DataFrame(results)\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_gpt = display_t_test(\"gpt_i_0\", file_paths)\n",
    "result_gpt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_bison = display_t_test(\"bison_i_0\", file_paths_bison)\n",
    "result_bison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_llama = display_t_test(\"llama_i_0\", file_paths_llama)\n",
    "result_llama"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_claude = display_t_test(\"claude_i_0\", file_paths_claude)\n",
    "result_claude"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
